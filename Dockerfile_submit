FROM spark

# Set default environment variables. These can also be set at the command line when invoking /bin/spark-submit
# ENV MASTER_CONTAINER_NAME=spark-master-local
# ENV SPARK_EXECUTOR_MEMORY=6G
# ENV SPARK_EXECUTOR_CORES=3
ENV HIVE_HOME=/opt/hive
ENV HUDI_HOME=/opt/hudi
ENV PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.9.5-src.zip:$PYTHONPATH
ENV PATH=$SPARK_HOME/bin:$SPARK_HOME/python:$PATH
# ENV PATH=$PATH:$HIVE_HOME/bin


RUN  mkdir -p "$HIVE_HOME" "$HUDI_HOME" 
RUN wget -qO - https://dlcdn.apache.org/hudi/0.13.1/hudi-0.13.1.src.tgz | tar -xz -C $HUDI_HOME --strip-components=1 
RUN wget -qO - https://dlcdn.apache.org/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz | tar -xz -C $HIVE_HOME --strip-components=1 
#Run jupyter notebook locally
# Run schematool for mysql
COPY data/hive-site.xml $HIVE_HOME/conf/
COPY apps/mysql-connector-java-8.0.14.jar $HIVE_HOME/libs/
RUN $HIVE_HOME/bin/schematool -dbType mysql -initSchema --verbose


ENV PYSPARK_DRIVER_PYTHON='jupyter'
ENV PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port=8889 --allow-root --ip=0.0.0.0 --NotebookApp.token='
# Install libraries
COPY ./resources/requirements.txt .
RUN update-alternatives --install /usr/bin/pip pip /usr/bin/pip3 1
RUN pip install -r requirements.txt

# Copy examples python files into container
COPY ./examples/ /home/examples/
COPY ./resources/log4j.properties $SPARK_HOME/conf/
# COPY ./resources/spark-defaults.conf $SPARK_HOME/conf/
EXPOSE 4040 8888 18080 8889 10000 10001

WORKDIR $SPARK_HOME
